# phd_speed_harmo_marl

## Deep Q-Learning Agent

Framework: Q-Learning with Deep Neural Network.

Context: speed recommendations for a road with 1-N lanes.

Environment: The physical objects, functional objects, and information flow are inherited from the [TM21: Speed Harmonization](https://www.arc-it.net/html/servicepackages/sp68.html#tab-3) service package defined in ARC-IT.
A link (road) with 1-N lanes that approaches areas of traffic congestion, bottlenecks, incidents, special events, and other conditions that affect the traffic flow. 
The roadway environment (ITS Roadway Equipment and Connected Vehicle Roadside Equipment) and the Vehicles participating in the traffic under discussion will provide the necessary inputs to the agents (Center, which is composed from Transportation Information Center and Traffic Management Center).
We will use the weather information, such as environmental conditions (temperature, wind, humidity, precipitation, and visibility), road surface conditions (temperature, moisture, icing, and salinity), and location and motion information of the connected vehicles passing the lane as syntethic data generated by a Python script. The agents will simulate the operational status of the sensors and the system as a whole, including fault data, which will limit the recommended speed to the default value.

Traffic generation: For every episode, **[tbd]** vehicles are created.

Agent (Speed Harmonisation System - SHS): **[tbd]**

State: **[tbd]**

Action: **[tbd]**

Reward: **[tbd]**

Learning mechanism: the agent make use of the Q-learning equation Q(s,a) = reward + gamma â€¢ max Q'(s',a') to update the action values and a deep neural network to learn the state-action function. The neural network is fully connected with **[tbd]** neurons as input (the state), **[tbd]** hidden layers of **[tbd]** neurons each, and the output layers with **[tbd]** neurons representing the **[tbd]** possible actions. Also, an experience replay mechanism is implemented: the experience of the agent is stored in a memory and, at the end of each episode, multiple batches of randomized samples are extracted from the memory and used to train the neural network, once the action values have been updated with the Q-learning equation.

## Components versions

* Python 3.7.9
  - keras
  - gymnasium
* TensorFlow 2.10
* CUDA 11.2 with cuDNN 8.1
* iota-sdk
* https://github.com/iotaledger/iota.py - "Sandbox" environment with pre-filled Tangle data for testing and development
